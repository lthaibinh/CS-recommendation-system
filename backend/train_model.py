# -*- coding: utf-8 -*-
"""main_ALS copy 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ee3hsl5xN91ldDdZhv0kX5sV80jlmECi
"""


# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from glob import glob
import os
from datetime import datetime, timezone
import warnings
warnings.filterwarnings('ignore')

# Set visualization style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)


# Load all CSV files from dataset folder
data_path = 'dataset/'
csv_files = sorted(glob(os.path.join(data_path, 'label_full_data_part_*.csv')))


# Load and combine all CSV files
df_list = []
for file in csv_files:
    df_temp = pd.read_csv(file)
    df_list.append(df_temp)

# Combine all dataframes
df = pd.concat(df_list, ignore_index=True)


# Display first few rows




# Convert TimePlaced to datetime
df['TimePlaced'] = pd.to_datetime(df['TimePlaced'])

# Extract additional time features
df['Date'] = df['TimePlaced'].dt.date
df['Hour'] = df['TimePlaced'].dt.hour
df['DayOfWeek'] = df['TimePlaced'].dt.day_name()
df['Month'] = df['TimePlaced'].dt.month
df['Year'] = df['TimePlaced'].dt.year

# Statistical summary


# Additional statistics


"""# Data Visualization

"""

# 1. Price Distribution
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Histogram
axes[0].hist(df['Price'], bins=50, edgecolor='black', alpha=0.7)
axes[0].set_xlabel('Price')
axes[0].set_ylabel('Frequency')
axes[0].set_title('Price Distribution')
axes[0].grid(True, alpha=0.3)

# Box plot
axes[1].boxplot(df['Price'], vert=True)
axes[1].set_ylabel('Price')
axes[1].set_title('Price Box Plot')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()



# 2. Top 20 Products by Frequency
top_products = df['ProductId'].value_counts().head(20)

plt.figure(figsize=(12, 6))
top_products.plot(kind='bar', color='skyblue', edgecolor='black')
plt.xlabel('Product ID')
plt.ylabel('Frequency')
plt.title('Top 20 Most Frequently Ordered Products')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()


# 3. Top 20 Product Families
family_counts = df[df['Family_Id'] != 0]['Family_Id'].value_counts().head(20)

plt.figure(figsize=(12, 6))
family_counts.plot(kind='bar', color='lightcoral', edgecolor='black')
plt.xlabel('Family ID')
plt.ylabel('Frequency')
plt.title('Top 20 Most Frequent Product Families (excluding 0)')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()


# 4. Orders Over Time
orders_by_date = df.groupby('Date')['orderId'].nunique()

plt.figure(figsize=(15, 6))
plt.plot(orders_by_date.index, orders_by_date.values, marker='o', linewidth=2, markersize=4)
plt.xlabel('Date')
plt.ylabel('Number of Orders')
plt.title('Number of Orders Over Time')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()


# 5. Orders by Hour of Day
orders_by_hour = df.groupby('Hour')['orderId'].nunique()

plt.figure(figsize=(12, 6))
plt.bar(orders_by_hour.index, orders_by_hour.values, color='mediumseagreen', edgecolor='black')
plt.xlabel('Hour of Day')
plt.ylabel('Number of Orders')
plt.title('Orders Distribution by Hour of Day')
plt.xticks(range(0, 24))
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()



# 6. Orders by Day of Week
day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
orders_by_day = df.groupby('DayOfWeek')['orderId'].nunique()
orders_by_day = orders_by_day.reindex(day_order)

plt.figure(figsize=(12, 6))
plt.bar(orders_by_day.index, orders_by_day.values, color='plum', edgecolor='black')
plt.xlabel('Day of Week')
plt.ylabel('Number of Orders')
plt.title('Orders Distribution by Day of Week')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()



# 7. Revenue by Date
revenue_by_date = df.groupby('Date')['Price'].sum()

plt.figure(figsize=(15, 6))
plt.plot(revenue_by_date.index, revenue_by_date.values, marker='o', linewidth=2, markersize=4, color='orange')
plt.xlabel('Date')
plt.ylabel('Revenue ($)')
plt.title('Total Revenue Over Time')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()


# 8. Customer Analysis - Top 20 Customers by Number of Orders
customer_orders = df.groupby('Account_Id')['orderId'].nunique().sort_values(ascending=False).head(20)

plt.figure(figsize=(12, 6))
customer_orders.plot(kind='bar', color='teal', edgecolor='black')
plt.xlabel('Account ID')
plt.ylabel('Number of Orders')
plt.title('Top 20 Customers by Number of Orders')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()



# 9. Customer Analysis - Top 20 Customers by Total Spending
customer_spending = df.groupby('Account_Id')['Price'].sum().sort_values(ascending=False).head(20)

plt.figure(figsize=(12, 6))
customer_spending.plot(kind='bar', color='gold', edgecolor='black')
plt.xlabel('Account ID')
plt.ylabel('Total Spending ($)')
plt.title('Top 20 Customers by Total Spending')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()



# 10. Products per Order Distribution
products_per_order = df.groupby('orderId').size()

plt.figure(figsize=(12, 6))
plt.hist(products_per_order, bins=50, edgecolor='black', alpha=0.7, color='lightblue')
plt.xlabel('Number of Products per Order')
plt.ylabel('Frequency')
plt.title('Distribution of Products per Order')
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()


# 11. Correlation Heatmap
# Create correlation matrix for numerical columns
corr_data = df[['ProductId', 'Price', 'Family_Id', 'Hour', 'Month']].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(corr_data, annot=True, cmap='coolwarm', center=0, fmt='.2f',
            square=True, linewidths=1, cbar_kws={"shrink": 0.8})
plt.title('Correlation Matrix of Numerical Features')
plt.tight_layout()
plt.show()

"""# Data Summary for ML Model

"""


customer_stats = df.groupby('Account_Id').agg({
    'orderId': 'nunique',
    'Price': 'sum'
})

# Install required libraries for BPR model

"""### ‚ö†Ô∏è Important Note: ALS Model Usage in `implicit` Library

**Correct Usage for ALS:**

The ALS implementation in `implicit` expects **user√óitem** matrices for both training and inference:


**Key Points:**
- **DO NOT transpose** the matrix for ALS (unlike some other libraries)
- Rows represent users, columns represent items
- For 21,202 users and 24,232 items: shape is **(21202, 24232)**
- The model treats the first dimension as users, second as items

B∆∞·ªõc 0: Kh·ªüi t·∫°o Spark
"""


from pyspark.sql import SparkSession
import pyspark.sql.functions as F

spark = SparkSession.builder \
    .master("local[*]") \
    .appName("ALS_Recommender_Master_Project") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()


"""B∆∞·ªõc 1: Chu·∫©n b·ªã D·ªØ li·ªáu (Aggregation) ‚öôÔ∏è

D·ªØ li·ªáu 2.5 tri·ªáu d√≤ng c·ªßa b·∫°n l√† m·ªôt log giao d·ªãch. M√¥ h√¨nh ALS kh√¥ng ƒë·ªçc ƒë∆∞·ª£c log, n√≥ c·∫ßn m·ªôt ma tr·∫≠n (User, Item, Rating). Ch√∫ng ta c·∫ßn t·∫°o ra c·ªôt "Rating" ng·∫ßm ƒë·ªãnh.

Ch√∫ng ta s·∫Ω kh√¥ng d√πng Price l√†m rating. Thay v√†o ƒë√≥, ch√∫ng ta s·∫Ω ƒë·∫øm s·ªë l·∫ßn mua (purchase_count). ƒê√¢y l√† t√≠n hi·ªáu "ƒë·ªô tin c·∫≠y" (confidence) c·ªßa ch√∫ng ta. M·ªôt ng∆∞·ªùi mua 10 l·∫ßn th√¨ "th√≠ch" s·∫£n ph·∫©m ƒë√≥ h∆°n m·ªôt ng∆∞·ªùi mua 1 l·∫ßn.
"""

user_item_data = list(df[['Account_Id', 'ProductId']].itertuples(index=False, name=None))

columns = ["Account_Id", "ProductId"]
spark_df = spark.createDataFrame(user_item_data, columns)

data_agg = spark_df.groupBy("Account_Id", "ProductId") \
                   .agg(F.count("*").alias("purchase_count"))

data_agg.show()

"""B∆∞·ªõc 2: Ph√¢n chia D·ªØ li·ªáu (Train / Validation / Test)"""

(train_data, validation_data, test_data) = data_agg.randomSplit([0.7, 0.15, 0.15], seed=42)


"""B∆∞·ªõc 3: Kh·ªüi t·∫°o M√¥ h√¨nh ALS

3.1 Defind model
"""



from pyspark.ml.recommendation import ALS
from itertools import product
from pyspark.ml.evaluation import RankingEvaluator


# --- Chu·∫©n b·ªã "ƒê√°p √°n" (ch·ªâ l√†m 1 l·∫ßn) ---
true_items_val = validation_data \
    .groupBy("Account_Id") \
    .agg(F.collect_list("ProductId").alias("true_items"))

# --- Kh·ªüi t·∫°o Th∆∞·ªõc ƒëo (ch·ªâ l√†m 1 l·∫ßn) ---
evaluator = RankingEvaluator(
    metricName="ndcgAtK", k=10,
    labelCol="true_items", predictionCol="pred_items"
)

from pyspark.ml.evaluation import RankingEvaluator
import pyspark.sql.functions as F
from pyspark.sql.types import DoubleType

def calculate_metrics_optimized(model, validation_data, k_values=[10, 15, 20, 25, 30, 35, 40]):
    """
    Optimized: Calculates metrics by generating recommendations ONLY ONCE.
    """
    # 1. Find the max K to generate recommendations just once
    max_k = max(k_values)
    print(f"--- Generating Top-{max_k} recommendations (One-time calc) ---")
    
    # Generate for max_k
    recs = model.recommendForAllUsers(max_k)
    
    # Prepare Ground Truth (AND CAST TO DOUBLE)
    # RankingEvaluator requires Double types for IDs
    true_items_val = validation_data \
        .groupBy("Account_Id") \
        .agg(F.collect_list(F.col("ProductId").cast("double")).alias("true_items"))
    
    # Join Predictions with Ground Truth
    # We cast the predictions array to Double as well
    full_eval_df = recs.select("Account_Id", "recommendations.ProductId") \
        .join(true_items_val, "Account_Id") \
        .select(
            F.col("Account_Id"), 
            F.col("true_items"), 
            F.col("ProductId").cast("array<double>").alias("all_pred_items")
        )
    
    # CRITICAL: Cache this DataFrame so we don't re-compute the join/rec for every K
    full_eval_df.cache()
    
    # Force materialization of cache to ensure speed in loop
    print(f"Cached evaluation data for {full_eval_df.count()} users.")

    metrics_dict = {}

    for k in k_values:
        # 2. Slice the array efficiently using Spark SQL
        # slice(col, start, length) -> Note: Spark SQL arrays are 1-based index
        k_pred_col = F.slice(F.col("all_pred_items"), 1, k)
        
        # Create a temp DF for this specific K
        k_eval_df = full_eval_df.withColumn("prediction_at_k", k_pred_col)

        # 3. Calculate Metrics using Spark Native Evaluators
        
        # NDCG
        ndcg = RankingEvaluator(
            metricName="ndcgAtK", k=k, 
            labelCol="true_items", predictionCol="prediction_at_k"
        ).evaluate(k_eval_df)
        
        # MAP
        map_score = RankingEvaluator(
            metricName="meanAveragePrecisionAtK", k=k,
            labelCol="true_items", predictionCol="prediction_at_k"
        ).evaluate(k_eval_df)
        
        # Precision
        precision = RankingEvaluator(
            metricName="precisionAtK", k=k,
            labelCol="true_items", predictionCol="prediction_at_k"
        ).evaluate(k_eval_df)
        
        # Recall
        recall = RankingEvaluator(
            metricName="recallAtK", k=k,
            labelCol="true_items", predictionCol="prediction_at_k"
        ).evaluate(k_eval_df)

        # 4. Calculate Coverage and HitRate manually
        # These metrics require collecting all items, so we convert to pandas for calculation
        eval_pd = k_eval_df.select("Account_Id", "true_items", "prediction_at_k").toPandas()
        
        total_hits = 0
        total_users = len(eval_pd)
        all_recommended_items = set()
        all_items = set()
        
        for _, row in eval_pd.iterrows():
            true_set = set(row['true_items'])
            pred_set = set(row['prediction_at_k'])
            all_items.update(true_set)
            all_recommended_items.update(pred_set)
            
            # Count hits (items that are both recommended and in true items)
            if len(pred_set) > 0:
                hits = len(true_set & pred_set)
                if hits > 0:
                    total_hits += 1  # User has at least one hit
        
        # HitRate: proportion of users who have at least one hit
        hit_rate = total_hits / total_users if total_users > 0 else 0
        
        # Coverage: proportion of all items in catalog that are recommended
        coverage = len(all_recommended_items) / len(all_items) if len(all_items) > 0 else 0

        metrics_dict[k] = {
            'precision': precision,
            'recall': recall,
            'map': map_score,
            'ndcg': ndcg,
            'coverage': coverage,
            'hitRate': hit_rate
        }
        
        print(f"METRIC|k={k}|precision={precision:.4f}|recall={recall:.4f}|map={map_score:.4f}|ndcg={ndcg:.4f}|coverage={coverage:.4f}|hitRate={hit_rate:.4f}")

    # Clean up memory
    full_eval_df.unpersist()
    
    return metrics_dict
ranks = [10]
regParams = [0.01]
alphas = [1]
maxIters = [10]

best_model = None
best_ndcg = -1

true_items_val = validation_data \
    .groupBy("Account_Id") \
    .agg(F.collect_list("ProductId").alias("true_items"))

def train_and_eval(rank, regParam, alpha, maxIter, versionTag):
    als = ALS(
        userCol="Account_Id",
        itemCol="ProductId",
        ratingCol="purchase_count",

        # rank=r,
        # regParam=reg,
        # alpha=a,
        # coldStartStrategy="drop"

        # --- (Hyperparameters)

        # 1. implicitPrefs (Quy·∫øt ƒë·ªãnh H√†m Loss/T·ªëi ∆∞u h√≥a)
        # Ph·∫£i = True. B√°o cho ALS bi·∫øt: "ƒê√¢y l√† d·ªØ li·ªáu ng·∫ßm ƒë·ªãnh.
        # H√£y d√πng h√†m loss W-RMSE, kh√¥ng ph·∫£i RMSE.
        # H√£y hi·ªÉu r·∫±ng "purchase_count" l√† "confidence", kh√¥ng ph·∫£i "rating".
        implicitPrefs=True,

        # 2. rank (ƒê·ªô ph·ª©c t·∫°p c·ªßa m√¥ h√¨nh)
        # S·ªë l∆∞·ª£ng y·∫øu t·ªë ti·ªÅm ·∫©n (latent factors).
        # Qu√° nh·ªè (v√≠ d·ª•: 5) -> Underfitting (h·ªçc d·ªët)
        # Qu√° l·ªõn (v√≠ d·ª•: 100) -> Overfitting (h·ªçc v·∫πt)
        rank=rank,

        # 3. regParam (ƒêi·ªÅu chu·∫©n - "Li·ªÅu thu·ªëc" ch·ªØa Overfitting)
        # "Ph·∫°t" m√¥ h√¨nh n·∫øu n√≥ h·ªçc v·∫πt.
        # ƒê√¢y l√† tham s·ªë b·∫°n s·∫Ω "tune" nhi·ªÅu nh·∫•t tr√™n t·∫≠p Validation.
        regParam=regParam,

        # 4. alpha ("B·ªô khu·∫øch ƒë·∫°i" Confidence)
        # Khu·∫øch ƒë·∫°i s·ª± kh√°c bi·ªát gi·ªØa "mua 1 l·∫ßn" v√† "mua 50 l·∫ßn".
        # Nh∆∞ ƒë√£ v√≠ d·ª•: 1 + (alpha * purchase_count)
        alpha=alpha, # Gi√° tr·ªã 40 l√† m·∫∑c ƒë·ªãnh t·ªët cho e-commerce

        # 5. maxIter (S·ªë v√≤ng l·∫∑p t·ªëi ∆∞u h√≥a)
        # S·ªë l·∫ßn "ƒëi xu·ªëng n√∫i".
        maxIter=maxIter,

        # --- C√°c c√†i ƒë·∫∑t kh√°c ---
        coldStartStrategy="drop" # B·ªè qua user/item m·ªõi khi ƒë√°nh gi√°

    )
    model = als.fit(train_data)

    print("üíæ SAVING MODEL TO DISK")
    print("="*80)

     
    # Create models directory if it doesn't exist
    models_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "models")
    os.makedirs(models_dir, exist_ok=True)
    
    # Save model with version
    model_path = os.path.join(models_dir, f"als_model_{versionTag}")

    model.write().overwrite().save(model_path)

    # Calculate and log all metrics
    print("="*80)
    print("CALCULATING METRICS")
    print("="*80)
    metrics_dict = calculate_metrics_optimized(model, validation_data, k_values=[10, 15])
    
    # Also keep the old NDCG@10 log for backward compatibility
    if 10 in metrics_dict:
        ndcg_10 = metrics_dict[10]['ndcg']
        print(f"K·∫øt qu·∫£ (Validation) NDCG@10 = {ndcg_10:.4f} {rank}, {regParam}, {alpha}, {maxIter}")


if __name__ == "__main__":
    import argparse
    
    # Set up argument parser
    parser = argparse.ArgumentParser(description='ALS Recommender System Training')
    parser.add_argument('--rank', type=int, default=10, help='Number of latent factors (default: 10)')
    parser.add_argument('--regParam', type=float, default=0.01, help='Regularization parameter (default: 0.01)')
    parser.add_argument('--alpha', type=float, default=1, help='Confidence amplification factor (default: 1)')
    parser.add_argument('--maxIter', type=int, default=10, help='Maximum number of iterations (default: 10)')
    parser.add_argument('--versionTag', type=str, default=datetime.now(timezone.utc).strftime("%Y-%m-%d_%H-%M-%S"), help='Version tag for saving the model (e.g., a timestamp)')
    # Parse arguments
    args = parser.parse_args()
    
    # Extract values
    rank = args.rank
    regParam = args.regParam
    alpha = args.alpha
    maxIter = args.maxIter
    versionTag = args.versionTag
   
    print(f"Version tag: {versionTag}")
    # Train and evaluate model
    train_and_eval(rank, regParam, alpha, maxIter, versionTag)

